"""
–ö–æ–Ω—Ç–µ—Å—Ç –∫–∞—á–µ—Å—Ç–≤–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: –í–°–ï –º–æ–¥–µ–ª–∏ √ó –í–°–ï —Ñ–∞–π–ª—ã.

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏:
  - –ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –º–æ–¥–µ–ª–∏ —Å Ollama —Å–µ—Ä–≤–µ—Ä–∞
  - –ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ .docx —Ñ–∞–π–ª—ã –≤ test_docs/ –∏ –≤ RAG (data/temp/)
  - –ü—Ä–æ–≥–æ–Ω—è–µ—Ç –∫–∞–∂–¥—É—é –ø–∞—Ä—É (–º–æ–¥–µ–ª—å, —Ñ–∞–π–ª) —á–µ—Ä–µ–∑ /v1/completions
  - –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç LLM –æ—Ç–≤–µ—Ç —Å ground truth (—Å—Ç–∏–ª–∏ –∏–∑ docx)
  - –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç leaderboard –∏ –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç

–ó–∞–ø—É—Å–∫:
  poetry run python tests/test_formatting_quality.py
  poetry run python tests/test_formatting_quality.py --server http://localhost:8323 --ollama http://192.168.0.107:11434
  poetry run python tests/test_formatting_quality.py --file one_specific.docx   # —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ñ–∞–π–ª
  poetry run python tests/test_formatting_quality.py --model gemma3:12b         # —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å
"""

import sys
import os
import re
import json
import glob
import time
import argparse
import urllib.request
import urllib.error
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from difflib import SequenceMatcher
from collections import defaultdict
from typing import Any

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
BACKEND_ROOT = os.path.dirname(CURRENT_DIR)
if BACKEND_ROOT not in sys.path:
    sys.path.insert(0, BACKEND_ROOT)

try:
    from app.services.style_extractor import style_extractor
except ImportError as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
    sys.exit(1)

TEST_DOCS_DIR = os.path.join(CURRENT_DIR, "test_docs")
DATA_TEMP_DIR = os.path.join(BACKEND_ROOT, "data", "temp")
REPORTS_DIR = os.path.join(CURRENT_DIR, "reports")
os.makedirs(REPORTS_DIR, exist_ok=True)


# ============================================================================
# –ê–í–¢–û-–û–ë–ù–ê–†–£–ñ–ï–ù–ò–ï
# ============================================================================

def discover_models(server_url: str, ollama_url: str) -> list[str]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –í–°–ï–• –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å Ollama —á–µ—Ä–µ–∑ middleware."""
    url = f"{server_url.rstrip('/')}/api/tags"
    headers = {"X-Target-Ollama-Url": ollama_url}
    try:
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=10) as resp:
            data = json.loads(resp.read().decode())
            models = [m["name"] for m in data.get("models", [])]
            return models
    except Exception as e:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏: {e}")
        return []


def discover_docx_files(extra_dirs: list[str] | None = None) -> list[str]:
    """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ .docx —Ñ–∞–π–ª—ã –≤ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö."""
    search_dirs = [TEST_DOCS_DIR]
    if extra_dirs:
        search_dirs.extend(extra_dirs)

    found = []
    seen_names = set()
    for d in search_dirs:
        if not os.path.isdir(d):
            continue
        for path in glob.glob(os.path.join(d, "*.docx")):
            name = os.path.basename(path)
            if name not in seen_names and not name.startswith(".~lock"):
                seen_names.add(name)
                found.append(path)

    return sorted(found)


# ============================================================================
# GROUND TRUTH: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –í–°–ï–• –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –∏–∑ docx
# ============================================================================

def extract_ground_truth(docx_path: str) -> list[dict]:
    """
    –ü–∞—Ä—Å–∏—Ç docx —á–µ—Ä–µ–∑ style_extractor.
    –í—Å–µ –∞—Ç—Ä–∏–±—É—Ç—ã –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò –∏–∑ —Ç–µ–≥–æ–≤ [KEY: VALUE].
    """
    chunks = style_extractor.parse_docx(docx_path)
    records = []

    for chunk in chunks:
        text = chunk.get("text", "").strip()
        if not text or text == "<IMAGE_PLACEHOLDER>":
            continue

        meta = chunk.get("metadata", {})
        style_desc = chunk.get("style_desc", "")

        record: dict[str, Any] = {
            "text": text,
            "style_name": meta.get("style_name", "Normal"),
            "is_header": meta.get("is_header", False),
            "section_type": meta.get("section_type", "body"),
        }

        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –í–°–ï–• —Ç–µ–≥–æ–≤
        for m in re.finditer(r'\[([^:]+):\s*([^\]]+)\]', style_desc):
            record[f"tag_{m.group(1).strip()}"] = m.group(2).strip()

        records.append(record)

    return records


def extract_plain_text(docx_path: str, max_chars: int) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç plain text –∏–∑ docx, –æ–±—Ä–µ–∑–∞—è –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏."""
    chunks = style_extractor.parse_docx(docx_path)
    lines = [c["text"].strip() for c in chunks
             if c.get("text", "").strip() and c["text"] != "<IMAGE_PLACEHOLDER>"]
    text = "\n\n".join(lines)
    if len(text) > max_chars:
        text = text[:max_chars] + "\n... [TRUNCATED]"
    return text


# ============================================================================
# LLM –í–´–ó–û–í
# ============================================================================

def call_llm(
    text: str,
    model: str,
    server_url: str,
    ollama_url: str,
    timeout: int = 300,
) -> tuple[list[dict] | None, float]:
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ /v1/completions.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (parsed_list | None, elapsed_seconds).
    """
    url = f"{server_url.rstrip('/')}/v1/completions"
    payload = {
        "model": model,
        "prompt": f"=== USER CONTENT (CONTENT SOURCE) ===\n{text}",
        "stream": False,
        "format": "json",
        "options": {"num_ctx": 8192},
    }
    headers = {
        "Content-Type": "application/json",
        "X-Target-Ollama-Url": ollama_url,
    }
    req = urllib.request.Request(
        url, data=json.dumps(payload).encode(), headers=headers, method="POST"
    )

    start = time.time()
    try:
        with urllib.request.urlopen(req, timeout=timeout) as resp:
            data = json.loads(resp.read().decode())
    except Exception as e:
        return None, time.time() - start

    elapsed = time.time() - start
    raw = data.get("response", "")

    # –ü–∞—Ä—Å–∏–Ω–≥ JSON
    parsed = _try_parse_json_list(raw)
    return parsed, elapsed


def _try_parse_json_list(raw: str) -> list[dict] | None:
    """–ü—ã—Ç–∞–µ—Ç—Å—è –∏–∑–≤–ª–µ—á—å JSON list/dict –∏–∑ —Å—ã—Ä–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
    if not raw:
        return None

    # 1. –ü—Ä—è–º–æ–π –ø–∞—Ä—Å–∏–Ω–≥
    try:
        p = json.loads(raw)
        if isinstance(p, list):
            return p
        if isinstance(p, dict):
            return [p]
    except (json.JSONDecodeError, ValueError):
        pass

    # 2. –ü–æ–∏—Å–∫ –º–∞—Å—Å–∏–≤–∞
    s, e = raw.find('['), raw.rfind(']')
    if s != -1 and e > s:
        try:
            p = json.loads(raw[s:e + 1])
            if isinstance(p, list):
                return p
        except (json.JSONDecodeError, ValueError):
            pass

    # 3. –ü–æ–∏—Å–∫ –æ–±—ä–µ–∫—Ç–∞
    s, e = raw.find('{'), raw.rfind('}')
    if s != -1 and e > s:
        try:
            p = json.loads(raw[s:e + 1])
            if isinstance(p, dict):
                return [p]
        except (json.JSONDecodeError, ValueError):
            pass

    return None


# ============================================================================
# –°–†–ê–í–ù–ï–ù–ò–ï: –ø–æ–ª–Ω–æ—Å—Ç—å—é –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ
# ============================================================================

# –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏ LLM –∫–ª—é—á–µ–π ‚Üí GT –∫–ª—é—á–µ–π
_KNOWN_MAPPINGS = {
    "style_name": "style_name",
    "font_family": "tag_F",
    "font_size": "tag_P",
    "bold": "tag_B",
    "align": "tag_A",
    "type": "section_type",
}


def _normalize(val: Any) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è."""
    if val is None:
        return ""
    s = str(val).strip().lower()
    if s in ("true", "1", "yes"):
        return "true"
    if s in ("false", "0", "no"):
        return "false"
    s = re.sub(r'\s*\(\d+\)\s*$', '', s)  # "CENTER (1)" ‚Üí "center"
    # section_type ‚Üî type –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    _type_map = {"header": "header", "body": "paragraph", "paragraph": "paragraph"}
    if s in _type_map:
        s = _type_map[s]
    return s


def build_key_map(gt: list[dict], llm: list[dict]) -> dict[str, str]:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å—Ç—Ä–æ–∏—Ç –º–∞–ø–ø–∏–Ω–≥ LLM keys ‚Üí GT keys."""
    gt_keys = {k for r in gt for k in r.keys()} - {"text"}
    llm_keys = {k for r in llm for k in r.keys()} - {"text"}

    mapping = {}
    # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ
    for lk, gk in _KNOWN_MAPPINGS.items():
        if lk in llm_keys and gk in gt_keys:
            mapping[lk] = gk
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ
    for lk in llm_keys:
        if lk in mapping:
            continue
        if lk in gt_keys:
            mapping[lk] = lk
        elif f"tag_{lk}" in gt_keys:
            mapping[lk] = f"tag_{lk}"
    return mapping


def fuzzy_match(gt_text: str, llm_text: str) -> float:
    """Fuzzy matching –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤ (0..1)."""
    if not gt_text or not llm_text:
        return 0.0
    a = re.sub(r'\s+', ' ', gt_text.strip().lower())[:200]
    b = re.sub(r'\s+', ' ', llm_text.strip().lower())[:200]
    return SequenceMatcher(None, a, b).ratio()


def evaluate(
    gt_records: list[dict],
    llm_records: list[dict],
) -> dict:
    """
    –ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: matching —Ç–µ–∫—Å—Ç–æ–≤ + –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏.
    """
    key_map = build_key_map(gt_records, llm_records)

    # Matching
    matched = []
    used = set()
    for gt in gt_records:
        best_score, best_idx = 0.0, -1
        for i, llm in enumerate(llm_records):
            if i in used:
                continue
            sc = fuzzy_match(gt.get("text", ""), llm.get("text", ""))
            if sc > best_score:
                best_score, best_idx = sc, i
        if best_idx >= 0 and best_score > 0.3:
            used.add(best_idx)
            matched.append({"gt": gt, "llm": llm_records[best_idx], "score": best_score})

    # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ–∫—Ä—ã—Ç–∏—è
    result: dict[str, Any] = {
        "gt_count": len(gt_records),
        "llm_count": len(llm_records),
        "matched_count": len(matched),
        "text_coverage_pct": round(len(matched) / len(gt_records) * 100, 1) if gt_records else 0,
        "avg_text_similarity": round(
            sum(p["score"] for p in matched) / len(matched) * 100, 1
        ) if matched else 0,
        "key_map": key_map,
        "attributes": {},
    }

    # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–∞–∂–¥–æ–º—É –∞—Ç—Ä–∏–±—É—Ç—É
    for llm_key, gt_key in key_map.items():
        total, correct, examples = 0, 0, []
        for pair in matched:
            gt_val = pair["gt"].get(gt_key)
            if gt_val is None:
                continue
            llm_val = pair["llm"].get(llm_key)
            total += 1
            if _normalize(gt_val) == _normalize(llm_val):
                correct += 1
            else:
                if len(examples) < 3:
                    examples.append({
                        "text": pair["gt"].get("text", "")[:40],
                        "expected": str(gt_val),
                        "got": str(llm_val),
                    })
        acc = round(correct / total * 100, 1) if total else 0
        result["attributes"][llm_key] = {
            "accuracy": acc,
            "correct": correct,
            "total": total,
            "examples": examples,
        }

    # Overall (—Å—Ä–µ–¥–Ω–µ–µ –ø–æ –≤—Å–µ–º –∞—Ç—Ä–∏–±—É—Ç–∞–º)
    accs = [v["accuracy"] for v in result["attributes"].values() if v["total"] > 0]
    result["overall_score"] = round(sum(accs) / len(accs), 1) if accs else 0

    return result


def precompute_all_gt(
    files: list[str],
    max_chars: int,
    workers: int = 4,
) -> dict[str, dict]:
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ—Ç ground truth –∏ —Ç–µ–∫—Å—Ç –∏–∑ –í–°–ï–• —Ñ–∞–π–ª–æ–≤.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç {path: {"gt": [...], "text": "...", "error": None}} 
    """
    cache: dict[str, dict] = {}
    lock = threading.Lock()
    
    def _process_file(path: str) -> tuple[str, dict]:
        try:
            gt = extract_ground_truth(path)
            text = extract_plain_text(path, max_chars)
            return path, {"gt": gt, "text": text, "error": None}
        except Exception as e:
            return path, {"gt": [], "text": "", "error": str(e)}
    
    print(f"\n‚ö° –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ GT –¥–ª—è {len(files)} —Ñ–∞–π–ª–æ–≤ ({workers} –ø–æ—Ç–æ–∫–æ–≤)...")
    start = time.time()
    
    with ThreadPoolExecutor(max_workers=workers) as pool:
        futures = {pool.submit(_process_file, f): f for f in files}
        done = 0
        for future in as_completed(futures):
            path, result = future.result()
            with lock:
                cache[path] = result
                done += 1
            fname = os.path.basename(path)
            if result["error"]:
                print(f"  ‚ùå [{done}/{len(files)}] {fname}: {result['error']}")
            else:
                print(f"  ‚úÖ [{done}/{len(files)}] {fname}: {len(result['gt'])} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
    
    elapsed = time.time() - start
    ok_count = sum(1 for v in cache.values() if not v["error"])
    print(f"  ‚è±Ô∏è  GT –≥–æ—Ç–æ–≤ –∑–∞ {elapsed:.1f}s ({ok_count}/{len(files)} —Ñ–∞–π–ª–æ–≤ –û–ö)")
    
    return cache


def _run_single(
    model: str,
    file_path: str,
    gt_cache: dict[str, dict],
    server_url: str,
    ollama_url: str,
    timeout: int,
) -> dict:
    """
    –û–¥–∏–Ω –ø—Ä–æ–≥–æ–Ω: –º–æ–¥–µ–ª—å √ó —Ñ–∞–π–ª.
    GT –±–µ—Ä—ë—Ç—Å—è –∏–∑ –∫—ç—à–∞ (—É–∂–µ –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω).
    """
    fname = os.path.basename(file_path)
    cached = gt_cache.get(file_path, {})
    
    if cached.get("error"):
        return _error_result(model, fname, f"GT error: {cached['error']}")
    
    gt = cached.get("gt", [])
    text = cached.get("text", "")
    
    if not gt:
        return _error_result(model, fname, "Empty GT")
    
    # –í—ã–∑–æ–≤ LLM (—ç—Ç–æ –æ—Å–Ω–æ–≤–Ω–æ–π bottleneck)
    llm_records, elapsed = call_llm(text, model, server_url, ollama_url, timeout)
    
    if llm_records is None:
        return _error_result(model, fname, "No JSON", elapsed)
    
    # –û—Ü–µ–Ω–∫–∞
    metrics = evaluate(gt, llm_records)
    metrics["model"] = model
    metrics["file"] = fname
    metrics["elapsed_sec"] = round(elapsed, 1)
    metrics["status"] = "OK"
    
    return metrics


def run_contest(
    models: list[str],
    files: list[str],
    server_url: str,
    ollama_url: str,
    max_chars: int,
    timeout: int,
    workers: int = 4,
) -> list[dict]:
    """
    –ü—Ä–æ–≥–æ–Ω—è–µ—Ç –≤—Å–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ (–º–æ–¥–µ–ª—å √ó —Ñ–∞–π–ª).
    GT –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ, LLM –≤—ã–∑–æ–≤—ã ‚Äî –°–¢–†–û–ì–û –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û:
    –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ–º –æ–¥–∏–Ω —Ñ–∞–π–ª, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É.
    """
    # –§–∞–∑–∞ 1: –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ GT (CPU-bound, –±–µ–∑–æ–ø–∞—Å–Ω–æ)
    gt_cache = precompute_all_gt(files, max_chars, workers=workers)
    
    # –§–∞–∑–∞ 2: –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–´–ï LLM –≤—ã–∑–æ–≤—ã
    # –ü–æ—Ä—è–¥–æ–∫: –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞ –ø—Ä–æ–≥–æ–Ω—è–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏, –ø–æ—Ç–æ–º —Å–ª–µ–¥—É—é—â–∏–π —Ñ–∞–π–ª
    total_runs = len(models) * len(files)
    results = []
    start_time = time.time()
    done = 0
    
    print(f"\nüöÄ –ó–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ—Å—Ç–∞: {total_runs} –ø—Ä–æ–≥–æ–Ω–æ–≤ (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ)")
    
    for file_idx, file_path in enumerate(files, 1):
        fname = os.path.basename(file_path)
        print(f"\n{'='*60}")
        print(f"üìÑ [{file_idx}/{len(files)}] {fname}")
        print(f"{'='*60}")
        
        for model in models:
            done += 1
            
            result = _run_single(
                model, file_path, gt_cache,
                server_url, ollama_url, timeout
            )
            results.append(result)
            
            # –ü—Ä–æ–≥—Ä–µ—Å—Å + ETA
            elapsed_total = time.time() - start_time
            avg_per_run = elapsed_total / done
            eta = avg_per_run * (total_runs - done)
            
            status = result.get("status", "?")
            if status == "OK":
                cov = result.get('text_coverage_pct', 0)
                score = result.get('overall_score', 0)
                elems = result.get('llm_count', 0)
                print(
                    f"  ‚úÖ [{done}/{total_runs}] ü§ñ {model} | "
                    f"Cov={cov}% Score={score}% Elems={elems} | "
                    f"ETA: {eta/60:.1f}min"
                )
            else:
                print(
                    f"  ‚ùå [{done}/{total_runs}] ü§ñ {model} | "
                    f"{status} | ETA: {eta/60:.1f}min"
                )
    
    total_elapsed = time.time() - start_time
    print(f"\n‚è±Ô∏è  –ö–æ–Ω—Ç–µ—Å—Ç –∑–∞–≤–µ—Ä—à—ë–Ω –∑–∞ {total_elapsed/60:.1f} –º–∏–Ω—É—Ç")
    
    return results


def _error_result(model: str, fname: str, error: str, elapsed: float = 0) -> dict:
    return {
        "model": model,
        "file": fname,
        "status": f"FAIL: {error}",
        "gt_count": 0,
        "llm_count": 0,
        "matched_count": 0,
        "text_coverage_pct": 0,
        "avg_text_similarity": 0,
        "overall_score": 0,
        "elapsed_sec": round(elapsed, 1),
        "attributes": {},
        "key_map": {},
    }


# ============================================================================
# –û–¢–ß–Å–¢: Leaderboard + –¥–µ—Ç–∞–ª–∏
# ============================================================================

def save_contest_report(results: list[dict], output_dir: str) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç Markdown leaderboard –∏ JSON –¥–∞–º–ø."""
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    md_path = os.path.join(output_dir, f"contest_{ts}.md")
    json_path = os.path.join(output_dir, f"contest_{ts}.json")

    # --- Leaderboard ---
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –º–æ–¥–µ–ª–∏: —Å—Ä–µ–¥–Ω–µ–µ –ø–æ –≤—Å–µ–º —Ñ–∞–π–ª–∞–º
    model_scores: dict[str, list] = defaultdict(list)
    for r in results:
        model_scores[r["model"]].append(r)

    leaderboard = []
    for model, runs in model_scores.items():
        ok_runs = [r for r in runs if r["status"] == "OK"]
        fail_count = len(runs) - len(ok_runs)
        avg_coverage = (
            sum(r["text_coverage_pct"] for r in ok_runs) / len(ok_runs)
            if ok_runs else 0
        )
        avg_overall = (
            sum(r["overall_score"] for r in ok_runs) / len(ok_runs)
            if ok_runs else 0
        )
        avg_elements = (
            sum(r["llm_count"] for r in ok_runs) / len(ok_runs)
            if ok_runs else 0
        )
        avg_time = (
            sum(r["elapsed_sec"] for r in ok_runs) / len(ok_runs)
            if ok_runs else 0
        )

        leaderboard.append({
            "model": model,
            "files_tested": len(runs),
            "files_ok": len(ok_runs),
            "files_failed": fail_count,
            "avg_coverage": round(avg_coverage, 1),
            "avg_overall": round(avg_overall, 1),
            "avg_elements": round(avg_elements, 1),
            "avg_time_sec": round(avg_time, 1),
        })

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: –ª—É—á—à–∏–µ —Å–≤–µ—Ä—Ö—É (coverage * overall)
    leaderboard.sort(key=lambda x: (x["avg_coverage"] * x["avg_overall"]), reverse=True)

    # --- Markdown ---
    lines = [
        f"# üèÜ Formatting Contest Report",
        f"**–î–∞—Ç–∞:** {datetime.now().strftime('%Y-%m-%d %H:%M')}",
        f"**–ú–æ–¥–µ–ª–µ–π:** {len(model_scores)} | **–§–∞–π–ª–æ–≤:** {len(set(r['file'] for r in results))} | "
        f"**–ü—Ä–æ–≥–æ–Ω–æ–≤:** {len(results)}",
        "",
        "## Leaderboard",
        "",
        "| # | –ú–æ–¥–µ–ª—å | –§–∞–π–ª–æ–≤ ‚úÖ/‚ùå | Avg Coverage | Avg Score | Avg Elements | Avg Time |",
        "|---|---|---|---|---|---|---|",
    ]

    for i, lb in enumerate(leaderboard, 1):
        medal = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i}."
        lines.append(
            f"| {medal} | `{lb['model']}` | {lb['files_ok']}/{lb['files_failed']} | "
            f"{lb['avg_coverage']:.1f}% | {lb['avg_overall']:.1f}% | "
            f"{lb['avg_elements']:.0f} | {lb['avg_time_sec']:.1f}s |"
        )

    # --- –î–µ—Ç–∞–ª–∏ –ø–æ –∫–∞–∂–¥–æ–º—É –ø—Ä–æ–≥–æ–Ω—É ---
    lines.extend(["", "## –î–µ—Ç–∞–ª–∏ –ø–æ –ø—Ä–æ–≥–æ–Ω–∞–º", ""])

    for r in results:
        status_icon = "‚úÖ" if r["status"] == "OK" else "‚ùå"
        lines.append(f"### {status_icon} `{r['model']}` √ó `{r['file']}`")

        if r["status"] != "OK":
            lines.append(f"**–°—Ç–∞—Ç—É—Å:** {r['status']}")
            lines.append("")
            continue

        lines.append(
            f"Coverage: {r['text_coverage_pct']}% | "
            f"Score: {r['overall_score']}% | "
            f"Elements: {r['llm_count']}/{r['gt_count']} | "
            f"Time: {r['elapsed_sec']}s"
        )

        if r.get("attributes"):
            lines.append("")
            lines.append("| –ê—Ç—Ä–∏–±—É—Ç | Accuracy | Correct/Total |")
            lines.append("|---|---|---|")
            for attr, info in sorted(r["attributes"].items(), key=lambda x: x[1]["accuracy"]):
                icon = "‚úÖ" if info["accuracy"] >= 80 else "‚ö†Ô∏è" if info["accuracy"] >= 50 else "‚ùå"
                lines.append(
                    f"| {icon} `{attr}` | {info['accuracy']:.1f}% | {info['correct']}/{info['total']} |"
                )

            # –ü—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ–∫ (–∫–æ–º–ø–∞–∫—Ç–Ω–æ)
            has_examples = any(info["examples"] for info in r["attributes"].values())
            if has_examples:
                lines.append("")
                lines.append("<details><summary>–ü—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ–∫</summary>")
                lines.append("")
                for attr, info in r["attributes"].items():
                    for ex in info["examples"]:
                        lines.append(f"- **{attr}**: `{ex['text']}...` ‚Äî –æ–∂–∏–¥–∞–ª–æ—Å—å `{ex['expected']}`, –ø–æ–ª—É—á–µ–Ω–æ `{ex['got']}`")
                lines.append("")
                lines.append("</details>")

        lines.append("")

    with open(md_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    # --- JSON (–±–µ–∑ examples –¥–ª—è –∫–æ–º–ø–∞–∫—Ç–Ω–æ—Å—Ç–∏) ---
    json_data = {
        "timestamp": datetime.now().isoformat(),
        "leaderboard": leaderboard,
        "runs": [{k: v for k, v in r.items() if k != "key_map"} for r in results],
    }
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)

    print(f"\nüìä Leaderboard: {md_path}")
    print(f"üì¶ JSON: {json_path}")

    # –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π leaderboard
    print(f"\n{'='*60}")
    print("üèÜ LEADERBOARD")
    print(f"{'='*60}")
    for i, lb in enumerate(leaderboard, 1):
        medal = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"  {i}."
        print(f"  {medal} {lb['model']:30s} Coverage={lb['avg_coverage']:5.1f}%  "
              f"Score={lb['avg_overall']:5.1f}%  Elements={lb['avg_elements']:5.0f}")

    return md_path


# ============================================================================
# MAIN
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="Formatting Quality Contest")
    parser.add_argument("--server", "-s", default="http://localhost:8323",
                        help="URL middleware —Å–µ—Ä–≤–µ—Ä–∞")
    parser.add_argument("--ollama", "-o", default="http://192.168.0.107:11434",
                        help="URL Ollama —Å–µ—Ä–≤–µ—Ä–∞")
    parser.add_argument("--model", "-m", default=None,
                        help="–ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –º–æ–¥–µ–ª—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî –≤—Å–µ)")
    parser.add_argument("--file", "-f", default=None,
                        help="–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π .docx —Ñ–∞–π–ª (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî –≤—Å–µ –∏–∑ test_docs/)")
    parser.add_argument("--timeout", "-t", type=int, default=300,
                        help="–¢–∞–π–º–∞—É—Ç –Ω–∞ –æ–¥–∏–Ω LLM –≤—ã–∑–æ–≤ (—Å–µ–∫—É–Ω–¥—ã)")
    parser.add_argument("--max-chars", type=int, default=12000,
                        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è LLM")
    parser.add_argument("--max-files", type=int, default=0,
                        help="–õ–∏–º–∏—Ç —Ñ–∞–π–ª–æ–≤ (0 = –±–µ–∑ –ª–∏–º–∏—Ç–∞)")
    parser.add_argument("--workers", "-w", type=int, default=4,
                        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤")
    args = parser.parse_args()

    print("üèÅ FORMATTING QUALITY CONTEST")
    print(f"   Server: {args.server}")
    print(f"   Ollama: {args.ollama}")

    # --- –ú–æ–¥–µ–ª–∏ ---
    if args.model:
        models = [args.model]
    else:
        models = discover_models(args.server, args.ollama)
        if not models:
            print("‚ùå –ú–æ–¥–µ–ª–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            sys.exit(1)

    print(f"\nü§ñ –ú–æ–¥–µ–ª–∏ ({len(models)}):")
    for m in models:
        print(f"   - {m}")

    # --- –§–∞–π–ª—ã ---
    if args.file:
        if os.path.exists(args.file):
            files = [args.file]
        else:
            # –ò—â–µ–º –≤ test_docs
            candidate = os.path.join(TEST_DOCS_DIR, args.file)
            if os.path.exists(candidate):
                files = [candidate]
            else:
                print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.file}")
                sys.exit(1)
    else:
        files = discover_docx_files([DATA_TEMP_DIR])
        if not files:
            print(f"‚ùå –§–∞–π–ª—ã .docx –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {TEST_DOCS_DIR}")
            sys.exit(1)

    if args.max_files > 0:
        files = files[:args.max_files]

    print(f"\nüìÑ –§–∞–π–ª—ã ({len(files)}):")
    for f in files:
        print(f"   - {os.path.basename(f)}")

    print(f"\nüìê –í—Å–µ–≥–æ –ø—Ä–æ–≥–æ–Ω–æ–≤: {len(models)} √ó {len(files)} = {len(models) * len(files)}")
    print(f"   Max chars: {args.max_chars} | Timeout: {args.timeout}s")

    # --- –ö–æ–Ω—Ç–µ—Å—Ç ---
    results = run_contest(
        models=models,
        files=files,
        server_url=args.server,
        ollama_url=args.ollama,
        max_chars=args.max_chars,
        timeout=args.timeout,
        workers=args.workers,
    )

    # --- –û—Ç—á—ë—Ç ---
    if results:
        save_contest_report(results, REPORTS_DIR)
    else:
        print("‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")


if __name__ == "__main__":
    main()
